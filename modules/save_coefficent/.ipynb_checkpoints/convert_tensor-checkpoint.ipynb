{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "\n",
    "model_dir = '/home/charles/Workspace/tensorflow_tutoral/Test/checkpoints'\n",
    "\n",
    "checkpoint_path = os.path.join(model_dir, \"model.ckpt-8201\")\n",
    "\n",
    "# print out all the tensors name\n",
    "# print_tensors_in_checkpoint_file(file_name=checkpoint_path, tensor_name='', all_tensors=False, all_tensor_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other way to read data from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.python import pywrap_tensorflow \n",
    "# read data from checkpoint file\n",
    "#reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path)\n",
    "#var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "# print tensor name and value\n",
    "#for var in var_to_shape_map:\n",
    "    #print(\"tensor_name:\", var)\n",
    "    #print(reader.get_tensor(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/charles/Workspace/tensorflow_tutoral/Test/checkpoints/model.ckpt-8201\n",
      "model restored:\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# restore tensors in trained model\n",
    "tf.reset_default_graph\n",
    "\n",
    "# create some variables\n",
    "# parameters -> ['self', 'shape', 'dtype', 'partition_info']\n",
    "w_conv_1 = tf.get_variable(\"w_conv_1\", shape=(5, 5, 1, 16))\n",
    "w_conv_2 = tf.get_variable(\"w_conv_2\", shape=(5, 5, 16, 36))\n",
    "w_fc_1 = tf.get_variable(\"w_fc_1\", shape=(1764, 128))\n",
    "w_output_layer = tf.get_variable(\"w_output_layer\", shape=(128, 10))\n",
    "\n",
    "b_conv_1 = tf.get_variable(\"b_conv_1\", shape=(16,))\n",
    "b_conv_2 = tf.get_variable(\"b_conv_2\", shape=(36,))\n",
    "b_fc_1 = tf.get_variable(\"b_fc_1\", shape=(128,))\n",
    "b_output_layer = tf.get_variable(\"b_output_layer\", shape=(10,))\n",
    "\n",
    "# add ops to save and resore all the variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# launch the model, use saver to restore variables from disk,\n",
    "# and do some work with the model\n",
    "with tf.Session() as sess:\n",
    "    # restore variales from disk\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    \n",
    "    # change tensor to ndarray\n",
    "    w_conv_1_arr = w_conv_1.eval()\n",
    "    w_conv_2_arr = w_conv_2.eval()\n",
    "    w_fc_1_arr = w_fc_1.eval()\n",
    "    w_output_layer_arr = w_output_layer.eval()\n",
    "    \n",
    "    b_conv_1_arr = b_conv_1.eval()\n",
    "    b_conv_2_arr = b_conv_2.eval()\n",
    "    b_fc_1_arr = b_fc_1.eval()\n",
    "    b_output_layer_arr = b_output_layer.eval()\n",
    "    print(\"model restored:\")\n",
    "    # print the value of the variables\n",
    "    #print(\"w_conv_1:\\n %s\"%w_conv_1)\n",
    "    #print(\"w_conv_1_arr:\\n %s\"%w_conv_1_arr)\n",
    "\n",
    "# tensor to numpy\n",
    "#w_conv_2_arr = w_conv_2.eval(session=sess)\n",
    "# numpy to tensor\n",
    "# w_conv_2 = tf.convert_to_tensor(w_conv_2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights of 1st convolution layer:\n",
      "    type: <class 'numpy.ndarray'>\n",
      "    dtype: float32\n",
      "    shape: (5, 5, 1, 16)\n",
      "biases of 1st convolution layer:\n",
      "    type: <class 'numpy.ndarray'>\n",
      "    dtype: float32\n",
      "    shape: (16,)\n"
     ]
    }
   ],
   "source": [
    "print(\"weights of 1st convolution layer:\")\n",
    "print(\"    type:\",type(w_conv_1_arr))\n",
    "print(\"    dtype:\", w_conv_1_arr.dtype)\n",
    "print(\"    shape:\", w_conv_1_arr.shape)\n",
    "\n",
    "print(\"biases of 1st convolution layer:\")\n",
    "print(\"    type:\",type(b_conv_1_arr))\n",
    "print(\"    dtype:\", b_conv_1_arr.dtype)\n",
    "print(\"    shape:\", b_conv_1_arr.shape)\n",
    "\n",
    "# shape=(filter_height, filter_width, in_channals, out_channels)\n",
    "# shape=(row of filter, column of filter, number of channals, number of kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file_dir = \"coefficient_file/\"\n",
    "model_name = \"model-ckpt-8201-v1/\"\n",
    "file_storage_path = os.path.join(file_dir, model_name)\n",
    "\n",
    "if not os.path.exists(file_storage_path):\n",
    "    os.makedirs(file_storage_path)\n",
    "    \n",
    "# write coefiicent by iterating each row of kernels, 1 parameter each row\n",
    "# w-> write, a->append\n",
    "file_txt=open(file_storage_path+\"w_conv_1.txt\", \"w\")\n",
    "file_csv=open(file_storage_path+\"w_conv_1.csv\", \"w\")\n",
    "\n",
    "# fistly, we itrating each kernel\n",
    "for i in range(w_conv_1_arr.shape[3]):\n",
    "    # secondly, we iterating each dimension of the kernel,\n",
    "    # cause we have a 3D conv kernel\n",
    "    for j in range(w_conv_1_arr.shape[2]):\n",
    "        pd.DataFrame(w_conv_1_arr[:, :, j, i]).to_csv(file_csv, index=False)\n",
    "        #thirdly, we itrating each row \n",
    "        for k in range(w_conv_1_arr.shape[0]):\n",
    "            # finally, we iterating each column\n",
    "            # pd.DataFrame(w_conv_1_arr[k, :, j, i]).to_csv(file_csv, index=False)\n",
    "            np.savetxt(file_txt, w_conv_1_arr[k, :, j, i])\n",
    "            #for l in range(w_conv_2_arr.shape[1]):\n",
    "                #np.savetxt(file, w_conv_2_arr[l, k, j, i])\n",
    "                #ValueError: Expected 1D or 2D array, got 0D array instead\n",
    "file_txt.close()\n",
    "file_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write coefiicent by iterating each row of kernels, 1 parameter each row\n",
    "file_txt=open(file_storage_path+\"w_conv_2.txt\", \"w\")\n",
    "file_csv=open(file_storage_path+\"w_conv_2.csv\", \"w\")\n",
    "\n",
    "# fistly, we itrating each kernel\n",
    "for i in range(w_conv_2_arr.shape[3]):\n",
    "    # secondly, we iterating each dimension of the kernel,\n",
    "    # cause we have a 3D conv kernel\n",
    "    for j in range(w_conv_2_arr.shape[2]):\n",
    "        # delete index\n",
    "        pd.DataFrame(w_conv_2_arr[:, :, j, i]).to_csv(file_csv, index=False)\n",
    "        #thirdly, we itrating each row \n",
    "        for k in range(w_conv_2_arr.shape[0]):\n",
    "            # finally, we iterating each column\n",
    "            # pd.DataFrame(w_conv_2_arr[k, :, j, i]).to_csv(file_csv, index=False)\n",
    "            np.savetxt(file_txt, w_conv_2_arr[k, :, j, i])\n",
    "            #for l in range(w_conv_2_arr.shape[1]):\n",
    "                #np.savetxt(file, w_conv_2_arr[l, k, j, i])\n",
    "                #ValueError: Expected 1D or 2D array, got 0D array instead\n",
    "file_txt.close()\n",
    "file_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write coefiicent by each kernel, 5 parameter each row\n",
    "file_txt=open(file_storage_path+\"w_conv_1_test.txt\", \"w\")\n",
    "for i in range(w_conv_1_arr.shape[3]):\n",
    "    for j in range(w_conv_1_arr.shape[2]):\n",
    "        np.savetxt(file_txt, w_conv_1_arr[:,:,j,i])\n",
    "file_txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write coefiicent by each kernel, 5 parameter each row\n",
    "file_txt=open(file_storage_path+\"w_conv_2_test.txt\", \"w\")\n",
    "for i in range(w_conv_2_arr.shape[3]):\n",
    "    for j in range(w_conv_2_arr.shape[2]):\n",
    "        np.savetxt(file_txt, w_conv_2_arr[:,:,j,i])\n",
    "file_txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_txt=open(file_storage_path+\"w_fc_1.txt\", \"w\")\n",
    "file_csv=open(file_storage_path+\"w_fc_1.csv\", \"w\")\n",
    "# delimiter=\"\\n\"换行。 否则将存储为一个单行。默认分隔符为空格\n",
    "np.savetxt(file_txt, w_fc_1_arr[:, :], delimiter=\"\\n\")\n",
    "pd.DataFrame(w_output_layer_arr).to_csv(file_csv, index=False)\n",
    "#for i in range(w_fc_1_arr.shape[0]):\n",
    "#    pd.DataFrame(w_fc_1_arr[i, :]).to_csv(file_csv, index=False)\n",
    "\n",
    "file_txt.close()\n",
    "file_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_txt=open(file_storage_path+\"w_output_layer.txt\", \"w\")\n",
    "file_csv=open(file_storage_path+\"w_output_layer.csv\", \"w\")\n",
    "np.savetxt(file_txt, w_output_layer_arr[:, :], delimiter=\"\\n\")\n",
    "pd.DataFrame(w_output_layer_arr).to_csv(file_csv, index=False)\n",
    "#for i in range(w_output_layer_arr.shape[0]):\n",
    "#    pd.DataFrame(w_output_layer_arr[i, :]).to_csv(file_csv, index=False)\n",
    "file_txt.close()\n",
    "file_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_txt=open(file_storage_path+\"b_conv_1.txt\", \"w\")\n",
    "file_csv=open(file_storage_path+\"b_conv_1.csv\", \"w\")\n",
    "np.savetxt(file_txt, b_conv_1_arr[:], delimiter=\"\\n\")\n",
    "pd.DataFrame(b_conv_1_arr).to_csv(file_csv, index=False)\n",
    "file_txt.close()\n",
    "file_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_txt=open(file_storage_path+\"b_conv_2.txt\", \"w\")\n",
    "file_csv=open(file_storage_path+\"b_conv_2.csv\", \"w\")\n",
    "np.savetxt(file_txt, b_conv_2_arr[:], delimiter=\"\\n\")\n",
    "pd.DataFrame(b_conv_2_arr).to_csv(file_csv, index=False)\n",
    "file_txt.close()\n",
    "file_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_txt=open(file_storage_path+\"b_fc_1.txt\", \"w\")\n",
    "file_csv=open(file_storage_path+\"b_fc_1.csv\", \"w\")\n",
    "# default parameter, newline=\"\\n\"\n",
    "np.savetxt(file_txt, b_fc_1_arr[:], delimiter=\"\\n\")\n",
    "pd.DataFrame(b_fc_1_arr).to_csv(file_csv, index=False)\n",
    "file_txt.close()\n",
    "file_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_txt=open(file_storage_path+\"b_output_layer.txt\", \"w\")\n",
    "file_csv=open(file_storage_path+\"b_output_layer.csv\", \"w\")\n",
    "np.savetxt(file_txt, b_output_layer_arr[:], delimiter=\"\\n\")\n",
    "pd.DataFrame(b_output_layer_arr).to_csv(file_csv, index=False)\n",
    "file_txt.close()\n",
    "file_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
